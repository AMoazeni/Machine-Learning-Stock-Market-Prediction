{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code and Jupyter Notebook](https://github.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/blob/master/Jupyter%20Notebook/ML%20-%20Stock%20Market%20Prediction.ipynb) for Stock Market Prediction!\n",
    "\n",
    "<br></br>\n",
    "This article explores a Machine Learning algorithm called Recurrent Neural Network (RNN), it's a common Deep Learning technique used for continuous data pattern recognition. Recurrent Neural Network take into account how data changes over time, it's typically used for time-series data (stock prices, sensor readings, etc). Recurrent Neural Network can also be used for video analysis.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "You are provided with a dataset consisting of stock prices for Google Inc, used to train a model and predict future stock prices as shown below.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/01%20-%20Google%20Stock%20Price%20Prediction.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "For improved predictions, you can train this model on stock price data for more companies in the same sector, region, subsidiaries, etc. Sentiment analysis of the web, news, and social media may also be useful in your predictions. The open-source developer Sentdex has created a really useful tool for [S&P 500 Sentiment Analysis](http://sentdex.com/financial-analysis/).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "\n",
    "<br></br>\n",
    "As we try to model Machine Learning to behave like brains, weights represent long-term memory in the Temporal Lobe. Recognition of patterns and images is done by the Occipital Lobe which works similar to Convolution Neural Networks. Recurrent Neural Networks are like short-term memory which remembers recent memory and can create context similar to the Frontal Lobe. The Parietal Lobe is responsible for spacial recognition like Botlzman Machines. Recurrent Neural Networks connect neurons to themselves through time, creating a feedback loop that preserves short-term and long-term memory awareness.\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/02%20-%20Brain%20Diagram.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The following diagram represents the old-school way to describe RNNs, which shows a Feedback Loop (temporal loop) structure that connects hidden layers to themselves and the output layer which gives them a short-term memory.\n",
    "\n",
    "\n",
    "### Compact Form Representation\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/03%20-%20Old%20RNN%20Representation.png\">\n",
    "\n",
    "\n",
    "\n",
    "### Expanded Form Representation\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/04%20-%20Expanded%20%20RNN%20Representation.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "A more modern representation shows the following RNN types and use examples: \n",
    "\n",
    "1. One-To-Many: Computer description of an image. CNN used to classify images and then RNN used to make sense of images and generate context.\n",
    "\n",
    "2. Many-To-One: Sentiment Analysis of text (gague the positivity or negativity of text)\n",
    "\n",
    "3. Many-to-Many: Google translate of language who's vocabulary changes based on the gender of the subject. Also subtitling of a movie.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/05%20-%20RNN%20Examples.png\" width=\"600\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Check out Andrej Karpathy's Blog (Director of AI at Tesla) on [Github](http://karpathy.github.io/) and [Medium](https://medium.com/@karpathy/).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is the movie script writen by an AI trained with an LSTM Recurrent Neural Network: [Sunspring by Benjamin the Artificial Intelligence](https://www.youtube.com/watch?v=LY7x2Ihqjmc).\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# RNN Gradient Problem (Expanding or Vanishing)\n",
    "\n",
    "<br></br>\n",
    "The gradient is used to update the weights in an RNN by looking back a certain number of user defined steps. The lower the gradient, the harder it is to update the weights (vanishing gradient) of nodes further back in time. Especially because previous layers are used as inputs for future layers. This means old neurons are training much slower that more current neurons. It's like a domino effect.\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/06%20-%20RNN%20Vanish%20Gradient.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Expanding Gradient Solutions\n",
    "\n",
    "\n",
    "### 1. Truncated Back-propagation\n",
    "Stop back-propagation after a certain point (not an optimal because not updating all the weights). Better than doing nothing which can produce an irrelevant network.\n",
    "\n",
    "\n",
    "### 2. Penalties\n",
    "The gradient can be penalized and artificially reduced.\n",
    "\n",
    "\n",
    "### 3. Gradient Clipping\n",
    "A maximum limit for the gradient which stops it from rising more.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Vanishing Gradient Solutions\n",
    "\n",
    "### 1. Weight Initialization\n",
    "You can be smart about how you initialize weights to minimize the vanishing gradient problem.\n",
    "\n",
    "\n",
    "### 2. Echo State Network\n",
    "Designed to solve vanishing gradient problem. It's a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned.\n",
    "\n",
    "\n",
    "### 3. Long Short-Term Memory Networks (LSTM)\n",
    "Most popular RNN structure to tackle this problem.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM\n",
    "\n",
    "<br></br>\n",
    "When the weight of an RNN gradient 'W_rec' is less than 1 we get Vanishing Gradient, when 'W_rec' is more than 1 we get Expanding Gradient, thus we can set 'W_rec = 1'.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/07%20-%20LSTM.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "- Circles represent Layers (Vectors).\n",
    "\n",
    "- 'C' represents Memory Cells Layers.\n",
    "\n",
    "- 'h' represents Output Layers (Hidden States).\n",
    "\n",
    "- 'X' represents Input Layers.\n",
    "\n",
    "- Lines represent values being transferred.\n",
    "\n",
    "- Concatenated lines represent pipelines running in parallel.\n",
    "\n",
    "- Forks are when Data is copied.\n",
    "\n",
    "- Pointwise Element-by-Element Operation (X) represents valves (from left-to-right: Forget Valve, Memory Valve, Output Valve).\n",
    "\n",
    "- Valves can be open, closed or partially open as decided by an Activation Function.\n",
    "\n",
    "- Pointwise Element-by-Element Operation (+) represent a Tee pipe joint, allowing stuff through if the corresponding valve is activated.\n",
    "\n",
    "- Pointwise Element-by-Element Operation (Tanh) Tangent function that outputs (values between -1 to 1).\n",
    "\n",
    "- Sigma Layer Operation Sigmoid Activation Function (values from 0 to 1).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/08%20-%20LSTM%20Cell.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Step 1\n",
    "\n",
    "New Value 'X_t' and value from previous node 'h_t-1' decide if the forget valve should be opened or closed (Sigmoid).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/09%20-%20LSTM%20Step%201.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Step 2\n",
    "\n",
    "New Value 'X_t' and value from Previous Node 'h_t-1'. Together they decide if the memory valve should be opened or closed (Sigmoid). To what extent to let values through (Tanh from -1 to 1).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/10%20-%20LSTM%20Step%202.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Step 3\n",
    "\n",
    "Decide the extent to which a memory cell 'C_t' should be updated from the previous memory cell 'C_t-1'. Forget and memory valves used to decide this. You can update memory completely, not at all or only partially.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/11%20-%20LSTM%20Step%203.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Step 4\n",
    "\n",
    "New value 'X_t' and value from previous node 'h_t-1' decides which part of the memory pipeline, and to what extent they will be used as an Output 'h_t'.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/12%20-%20LSTM%20Step%204.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Variation 1 (Add Peep holes)\n",
    "\n",
    "Sigmoid layer activation functions now have additional information about the current state of the Memory Cell. So valve decisions are made, taking into account memory cell state.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/13%20-%20LSTM%20Var%201.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Variation 2 (Connect Forget & Memory Valves)\n",
    "\n",
    "Forget and memory valves can make a combined decision. They're connected with a '-1' multiplier so one opens when the other closes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/14%20-%20LSTM%20Var%202.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# LSTM Variation 3 (GRU: Gated Recurring Units)\n",
    "\n",
    "The memory pipeline is replaced by the hidden pipeline. Simpler but less flexible in terms of how many things are being monitored and controlled.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Jupyter%20Notebook/Images/15%20-%20LSTM%20Var%203.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Code\n",
    "\n",
    "<br></br>\n",
    "Download the code and run it with 'Jupyter Notebook' or copy the code into the 'Spyder' IDE found in the [Anaconda Distribution](https://www.anaconda.com/download/). 'Spyder' is similar to MATLAB, it allows you to step through the code and examine the 'Variable Explorer' to see exactly how the data is parsed and analyzed. Jupyter Notebook also offers a [Jupyter Variable Explorer Extension](http://volderette.de/jupyter-notebook-variable-explorer/) which is quite useful for keeping track of variables.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Stock-Market-Prediction.git\n",
    "$ cd Machine-Learning-Stock-Market-Prediction\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/10/2012</td>\n",
       "      <td>313.70</td>\n",
       "      <td>315.72</td>\n",
       "      <td>307.30</td>\n",
       "      <td>621.43</td>\n",
       "      <td>8,824,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/11/2012</td>\n",
       "      <td>310.59</td>\n",
       "      <td>313.52</td>\n",
       "      <td>309.40</td>\n",
       "      <td>624.25</td>\n",
       "      <td>4,817,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/12/2012</td>\n",
       "      <td>314.43</td>\n",
       "      <td>315.26</td>\n",
       "      <td>312.08</td>\n",
       "      <td>627.92</td>\n",
       "      <td>3,764,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/13/2012</td>\n",
       "      <td>311.96</td>\n",
       "      <td>312.30</td>\n",
       "      <td>309.37</td>\n",
       "      <td>623.28</td>\n",
       "      <td>4,631,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/17/2012</td>\n",
       "      <td>314.81</td>\n",
       "      <td>314.81</td>\n",
       "      <td>311.67</td>\n",
       "      <td>626.86</td>\n",
       "      <td>3,832,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/18/2012</td>\n",
       "      <td>312.14</td>\n",
       "      <td>315.82</td>\n",
       "      <td>309.90</td>\n",
       "      <td>631.18</td>\n",
       "      <td>5,544,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/19/2012</td>\n",
       "      <td>319.30</td>\n",
       "      <td>319.30</td>\n",
       "      <td>314.55</td>\n",
       "      <td>637.82</td>\n",
       "      <td>12,657,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/20/2012</td>\n",
       "      <td>294.16</td>\n",
       "      <td>294.40</td>\n",
       "      <td>289.76</td>\n",
       "      <td>584.39</td>\n",
       "      <td>21,231,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/23/2012</td>\n",
       "      <td>291.91</td>\n",
       "      <td>293.23</td>\n",
       "      <td>290.49</td>\n",
       "      <td>583.92</td>\n",
       "      <td>6,851,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/24/2012</td>\n",
       "      <td>292.07</td>\n",
       "      <td>292.74</td>\n",
       "      <td>287.92</td>\n",
       "      <td>579.34</td>\n",
       "      <td>6,134,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/25/2012</td>\n",
       "      <td>287.68</td>\n",
       "      <td>288.27</td>\n",
       "      <td>282.13</td>\n",
       "      <td>567.93</td>\n",
       "      <td>10,012,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2012</td>\n",
       "      <td>284.92</td>\n",
       "      <td>286.17</td>\n",
       "      <td>281.22</td>\n",
       "      <td>566.54</td>\n",
       "      <td>6,476,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2012</td>\n",
       "      <td>284.32</td>\n",
       "      <td>289.08</td>\n",
       "      <td>283.60</td>\n",
       "      <td>578.39</td>\n",
       "      <td>7,262,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2012</td>\n",
       "      <td>287.95</td>\n",
       "      <td>288.92</td>\n",
       "      <td>285.63</td>\n",
       "      <td>576.11</td>\n",
       "      <td>4,678,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2012</td>\n",
       "      <td>290.41</td>\n",
       "      <td>290.91</td>\n",
       "      <td>286.50</td>\n",
       "      <td>578.52</td>\n",
       "      <td>4,300,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2/1/2012</td>\n",
       "      <td>291.38</td>\n",
       "      <td>291.66</td>\n",
       "      <td>288.49</td>\n",
       "      <td>579.24</td>\n",
       "      <td>4,658,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2/2/2012</td>\n",
       "      <td>291.34</td>\n",
       "      <td>292.11</td>\n",
       "      <td>289.95</td>\n",
       "      <td>583.51</td>\n",
       "      <td>4,847,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2/3/2012</td>\n",
       "      <td>294.23</td>\n",
       "      <td>297.42</td>\n",
       "      <td>292.93</td>\n",
       "      <td>594.7</td>\n",
       "      <td>6,360,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2/6/2012</td>\n",
       "      <td>296.39</td>\n",
       "      <td>304.27</td>\n",
       "      <td>295.90</td>\n",
       "      <td>607.42</td>\n",
       "      <td>7,386,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2/7/2012</td>\n",
       "      <td>302.44</td>\n",
       "      <td>303.56</td>\n",
       "      <td>300.75</td>\n",
       "      <td>605.11</td>\n",
       "      <td>4,199,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2/8/2012</td>\n",
       "      <td>303.18</td>\n",
       "      <td>304.53</td>\n",
       "      <td>301.24</td>\n",
       "      <td>608.18</td>\n",
       "      <td>3,686,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2/9/2012</td>\n",
       "      <td>304.87</td>\n",
       "      <td>306.10</td>\n",
       "      <td>303.36</td>\n",
       "      <td>609.79</td>\n",
       "      <td>4,546,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2/10/2012</td>\n",
       "      <td>302.81</td>\n",
       "      <td>302.93</td>\n",
       "      <td>300.87</td>\n",
       "      <td>604.25</td>\n",
       "      <td>4,667,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2/13/2012</td>\n",
       "      <td>304.11</td>\n",
       "      <td>305.77</td>\n",
       "      <td>303.87</td>\n",
       "      <td>610.52</td>\n",
       "      <td>3,646,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2/14/2012</td>\n",
       "      <td>304.63</td>\n",
       "      <td>304.86</td>\n",
       "      <td>301.25</td>\n",
       "      <td>608.09</td>\n",
       "      <td>3,620,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>766.92</td>\n",
       "      <td>772.70</td>\n",
       "      <td>764.23</td>\n",
       "      <td>771.23</td>\n",
       "      <td>1,304,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>11/18/2016</td>\n",
       "      <td>771.37</td>\n",
       "      <td>775.00</td>\n",
       "      <td>760.00</td>\n",
       "      <td>760.54</td>\n",
       "      <td>1,547,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>762.61</td>\n",
       "      <td>769.70</td>\n",
       "      <td>760.60</td>\n",
       "      <td>769.2</td>\n",
       "      <td>1,330,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>11/22/2016</td>\n",
       "      <td>772.63</td>\n",
       "      <td>776.96</td>\n",
       "      <td>767.00</td>\n",
       "      <td>768.27</td>\n",
       "      <td>1,593,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>11/23/2016</td>\n",
       "      <td>767.73</td>\n",
       "      <td>768.28</td>\n",
       "      <td>755.25</td>\n",
       "      <td>760.99</td>\n",
       "      <td>1,478,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>11/25/2016</td>\n",
       "      <td>764.26</td>\n",
       "      <td>765.00</td>\n",
       "      <td>760.52</td>\n",
       "      <td>761.68</td>\n",
       "      <td>587,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>11/28/2016</td>\n",
       "      <td>760.00</td>\n",
       "      <td>779.53</td>\n",
       "      <td>759.80</td>\n",
       "      <td>768.24</td>\n",
       "      <td>2,188,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>11/29/2016</td>\n",
       "      <td>771.53</td>\n",
       "      <td>778.50</td>\n",
       "      <td>768.24</td>\n",
       "      <td>770.84</td>\n",
       "      <td>1,616,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>11/30/2016</td>\n",
       "      <td>770.07</td>\n",
       "      <td>772.99</td>\n",
       "      <td>754.83</td>\n",
       "      <td>758.04</td>\n",
       "      <td>2,392,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>12/1/2016</td>\n",
       "      <td>757.44</td>\n",
       "      <td>759.85</td>\n",
       "      <td>737.03</td>\n",
       "      <td>747.92</td>\n",
       "      <td>3,017,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>744.59</td>\n",
       "      <td>754.00</td>\n",
       "      <td>743.10</td>\n",
       "      <td>750.5</td>\n",
       "      <td>1,452,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>12/5/2016</td>\n",
       "      <td>757.71</td>\n",
       "      <td>763.90</td>\n",
       "      <td>752.90</td>\n",
       "      <td>762.52</td>\n",
       "      <td>1,394,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>12/6/2016</td>\n",
       "      <td>764.73</td>\n",
       "      <td>768.83</td>\n",
       "      <td>757.34</td>\n",
       "      <td>759.11</td>\n",
       "      <td>1,690,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>12/7/2016</td>\n",
       "      <td>761.00</td>\n",
       "      <td>771.36</td>\n",
       "      <td>755.80</td>\n",
       "      <td>771.19</td>\n",
       "      <td>1,761,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>12/8/2016</td>\n",
       "      <td>772.48</td>\n",
       "      <td>778.18</td>\n",
       "      <td>767.23</td>\n",
       "      <td>776.42</td>\n",
       "      <td>1,488,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>12/9/2016</td>\n",
       "      <td>780.00</td>\n",
       "      <td>789.43</td>\n",
       "      <td>779.02</td>\n",
       "      <td>789.29</td>\n",
       "      <td>1,821,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>12/12/2016</td>\n",
       "      <td>785.04</td>\n",
       "      <td>791.25</td>\n",
       "      <td>784.35</td>\n",
       "      <td>789.27</td>\n",
       "      <td>2,104,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>12/13/2016</td>\n",
       "      <td>793.90</td>\n",
       "      <td>804.38</td>\n",
       "      <td>793.34</td>\n",
       "      <td>796.1</td>\n",
       "      <td>2,145,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>12/14/2016</td>\n",
       "      <td>797.40</td>\n",
       "      <td>804.00</td>\n",
       "      <td>794.01</td>\n",
       "      <td>797.07</td>\n",
       "      <td>1,704,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>12/15/2016</td>\n",
       "      <td>797.34</td>\n",
       "      <td>803.00</td>\n",
       "      <td>792.92</td>\n",
       "      <td>797.85</td>\n",
       "      <td>1,626,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>12/16/2016</td>\n",
       "      <td>800.40</td>\n",
       "      <td>800.86</td>\n",
       "      <td>790.29</td>\n",
       "      <td>790.8</td>\n",
       "      <td>2,443,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>12/19/2016</td>\n",
       "      <td>790.22</td>\n",
       "      <td>797.66</td>\n",
       "      <td>786.27</td>\n",
       "      <td>794.2</td>\n",
       "      <td>1,232,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>12/20/2016</td>\n",
       "      <td>796.76</td>\n",
       "      <td>798.65</td>\n",
       "      <td>793.27</td>\n",
       "      <td>796.42</td>\n",
       "      <td>951,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>12/21/2016</td>\n",
       "      <td>795.84</td>\n",
       "      <td>796.68</td>\n",
       "      <td>787.10</td>\n",
       "      <td>794.56</td>\n",
       "      <td>1,211,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>12/22/2016</td>\n",
       "      <td>792.36</td>\n",
       "      <td>793.32</td>\n",
       "      <td>788.58</td>\n",
       "      <td>791.26</td>\n",
       "      <td>972,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "5      1/10/2012  313.70  315.72  307.30  621.43   8,824,000\n",
       "6      1/11/2012  310.59  313.52  309.40  624.25   4,817,800\n",
       "7      1/12/2012  314.43  315.26  312.08  627.92   3,764,400\n",
       "8      1/13/2012  311.96  312.30  309.37  623.28   4,631,800\n",
       "9      1/17/2012  314.81  314.81  311.67  626.86   3,832,800\n",
       "10     1/18/2012  312.14  315.82  309.90  631.18   5,544,000\n",
       "11     1/19/2012  319.30  319.30  314.55  637.82  12,657,800\n",
       "12     1/20/2012  294.16  294.40  289.76  584.39  21,231,800\n",
       "13     1/23/2012  291.91  293.23  290.49  583.92   6,851,300\n",
       "14     1/24/2012  292.07  292.74  287.92  579.34   6,134,400\n",
       "15     1/25/2012  287.68  288.27  282.13  567.93  10,012,700\n",
       "16     1/26/2012  284.92  286.17  281.22  566.54   6,476,500\n",
       "17     1/27/2012  284.32  289.08  283.60  578.39   7,262,000\n",
       "18     1/30/2012  287.95  288.92  285.63  576.11   4,678,400\n",
       "19     1/31/2012  290.41  290.91  286.50  578.52   4,300,700\n",
       "20      2/1/2012  291.38  291.66  288.49  579.24   4,658,700\n",
       "21      2/2/2012  291.34  292.11  289.95  583.51   4,847,400\n",
       "22      2/3/2012  294.23  297.42  292.93   594.7   6,360,700\n",
       "23      2/6/2012  296.39  304.27  295.90  607.42   7,386,700\n",
       "24      2/7/2012  302.44  303.56  300.75  605.11   4,199,700\n",
       "25      2/8/2012  303.18  304.53  301.24  608.18   3,686,400\n",
       "26      2/9/2012  304.87  306.10  303.36  609.79   4,546,300\n",
       "27     2/10/2012  302.81  302.93  300.87  604.25   4,667,700\n",
       "28     2/13/2012  304.11  305.77  303.87  610.52   3,646,100\n",
       "29     2/14/2012  304.63  304.86  301.25  608.09   3,620,900\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1228  11/17/2016  766.92  772.70  764.23  771.23   1,304,000\n",
       "1229  11/18/2016  771.37  775.00  760.00  760.54   1,547,100\n",
       "1230  11/21/2016  762.61  769.70  760.60   769.2   1,330,600\n",
       "1231  11/22/2016  772.63  776.96  767.00  768.27   1,593,100\n",
       "1232  11/23/2016  767.73  768.28  755.25  760.99   1,478,400\n",
       "1233  11/25/2016  764.26  765.00  760.52  761.68     587,400\n",
       "1234  11/28/2016  760.00  779.53  759.80  768.24   2,188,200\n",
       "1235  11/29/2016  771.53  778.50  768.24  770.84   1,616,600\n",
       "1236  11/30/2016  770.07  772.99  754.83  758.04   2,392,900\n",
       "1237   12/1/2016  757.44  759.85  737.03  747.92   3,017,900\n",
       "1238   12/2/2016  744.59  754.00  743.10   750.5   1,452,500\n",
       "1239   12/5/2016  757.71  763.90  752.90  762.52   1,394,200\n",
       "1240   12/6/2016  764.73  768.83  757.34  759.11   1,690,700\n",
       "1241   12/7/2016  761.00  771.36  755.80  771.19   1,761,000\n",
       "1242   12/8/2016  772.48  778.18  767.23  776.42   1,488,100\n",
       "1243   12/9/2016  780.00  789.43  779.02  789.29   1,821,900\n",
       "1244  12/12/2016  785.04  791.25  784.35  789.27   2,104,100\n",
       "1245  12/13/2016  793.90  804.38  793.34   796.1   2,145,200\n",
       "1246  12/14/2016  797.40  804.00  794.01  797.07   1,704,200\n",
       "1247  12/15/2016  797.34  803.00  792.92  797.85   1,626,500\n",
       "1248  12/16/2016  800.40  800.86  790.29   790.8   2,443,800\n",
       "1249  12/19/2016  790.22  797.66  786.27   794.2   1,232,100\n",
       "1250  12/20/2016  796.76  798.65  793.27  796.42     951,000\n",
       "1251  12/21/2016  795.84  796.68  787.10  794.56   1,211,300\n",
       "1252  12/22/2016  792.36  793.32  788.58  791.26     972,200\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Data/Google_Stock_Price_Train.csv')\n",
    "# '.values' need the 2nd Column Opening Price as a Numpy array (not vector)\n",
    "# '1:2' is used because the upper bound is ignored\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "\n",
    "# Feature Scaling\n",
    "# Use Normalization (versus Standardization) for RNNs with Sigmoid Activation Functions\n",
    "# 'MinMaxScalar' is a Normalization Library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 'feature_range = (0,1)' makes sure that training data is scaled to have values between 0 and 1\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "# Creating a data structure with 60 timesteps (look back 60 days) and 1 output\n",
    "# This tells the RNN what to remember (Number of timesteps) when predicting the next Stock Price\n",
    "# The wrong number of timesteps can lead to Overfitting or bogus results\n",
    "# 'x_train' Input with 60 previous days' stock prices\n",
    "X_train = []\n",
    "# 'y_train' Output with next day's stock price\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping (add more dimensions)\n",
    "# This lets you add more indicators that may potentially have corelation with Stock Prices\n",
    "# Keras RNNs expects an input shape (Batch Size, Timesteps, input_dim)\n",
    "# '.shape[0]' is the number of Rows (Batch Size)\n",
    "# '.shape[1]' is the number of Columns (timesteps)\n",
    "# 'input_dim' is the number of factors that may affect stock prices\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Show the dataset we're working with\n",
    "display(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 0.0553\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0061\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0055\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0048\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0050\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0048\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0048\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0045\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0042\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0044\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0045\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0044\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0040\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0037\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0041\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0044\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0037\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0038\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0037\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0033\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0041\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0037\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0031\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0032\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0032\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0029\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0029\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0035\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0032\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0030\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0030\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0031\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0028\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0030\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0034\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0028\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0028\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0027\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0025\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0028\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0026\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0029\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0024\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0025\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0024\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0026\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0023\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0025\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0024\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0024\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0024\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 0.0018\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0021\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0021\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0020\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0021\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0019\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0021\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0018\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0019\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0021\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0019\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0020\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0018\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0019\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0018\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0017\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0015\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0017\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0015\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0015\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30471fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Building the RNN\n",
    "# Building a robust stacked LSTM with dropout regularization\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "# Regression is when you predict a continuous value\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "# 'units' is the number of LSTM Memory Cells (Neurons) for higher dimensionality\n",
    "# 'return_sequences = True' because we will add more stacked LSTM Layers\n",
    "# 'input_shape' of x_train\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "# 20% of Neurons will be ignored (10 out of 50 Neurons) to prevent Overfitting\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "# Not need to specify input_shape for second Layer, it knows that we have 50 Neurons from the previous layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# This is the last LSTM Layer. 'return_sequences = false' by default so we leave it out.\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "# 'units = 1' because Output layer has one dimension\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "# Keras documentation recommends 'RMSprop' as a good optimizer for RNNs\n",
    "# Trial and error suggests that 'adam' optimizer is a good choice\n",
    "# loss = 'mean_squared_error' which is good for Regression vs. 'Binary Cross Entropy' previously used for Classification\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "# 'X_train' Independent variables\n",
    "# 'y_train' Output Truths that we compare X_train to.\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Data/Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "\n",
    "# Getting the predicted stock price of 2017\n",
    "# We need 60 previous inputs for each day of the Test_set in 2017\n",
    "# Combine 'dataset_train' and 'dataset_test'\n",
    "# 'axis = 0' for Vertical Concatenation to add rows to the bottom\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "# Extract Stock Prices for Test time period, plus 60 days previous\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "# 'reshape' function to get it into a NumPy format\n",
    "inputs = inputs.reshape(-1,1)\n",
    "# Inputs need to be scaled to match the model trained on Scaled Feature\n",
    "inputs = sc.transform(inputs)\n",
    "# The following is pasted from above and modified for Testing, romove all 'Ys'\n",
    "X_test = []\n",
    "\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "# We need a 3D input so add another dimension\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "# Predict the Stock Price\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "# We need to inverse the scaling of our prediction to get a Dollar amount\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "388px",
    "left": "877px",
    "right": "20px",
    "top": "120px",
    "width": "543px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
